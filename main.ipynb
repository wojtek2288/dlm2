{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandab login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from huggingface_hub import notebook_login\n",
    "from enum import Enum\n",
    "import random\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    FacebookWav2Vec2 = 1\n",
    "    AudioSpectrogramTransformer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "SPLIT_SILENCE = False\n",
    "SAVE_TRAIN_TEST = False\n",
    "DATASET_PATH = 'train/audio'\n",
    "LEARNING_RATE = 3e-5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "LOGGING_STEPS = 10\n",
    "MODEL = Model.FacebookWav2Vec2\n",
    "MODEL_NAMES = { Model.FacebookWav2Vec2: \"Wav2Vec\", Model.AudioSpectrogramTransformer: \"AST\" }\n",
    "MODEL_NAME = MODEL_NAMES[MODEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ecb415b990>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "def split_audio(file_path, output_folder, k):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    length_ms = len(audio)\n",
    "\n",
    "    for i in range(0, length_ms, 1000):\n",
    "        end = i + 1000\n",
    "\n",
    "        if end > length_ms:\n",
    "            end = length_ms\n",
    "\n",
    "        chunk = audio[i:end]\n",
    "        chunk_name = f\"{output_folder}/chunk_{k:03d}.wav\"\n",
    "        chunk.export(chunk_name, format=\"wav\")\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    print(f\"Audio split into {length_ms//1000} chunks.\")\n",
    "\n",
    "    return k\n",
    "\n",
    "if SPLIT_SILENCE:\n",
    "    for file in os.listdir(f\"{DATASET_PATH}/_background_noise_/\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            k = split_audio(f\"{DATASET_PATH}/_background_noise_/{file}\", \"{DATASET_PATH}/silence\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db9d7e4d00f443c8f8f2dfede0830af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (C:/Users/User/.cache/huggingface/datasets/audiofolder/audio-51c9dfcf18469dc6/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n",
      "Loading cached processed dataset at C:\\Users\\User\\.cache\\huggingface\\datasets\\audiofolder\\audio-51c9dfcf18469dc6\\0.0.0\\6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc\\cache-6ebeeea247822a5f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 52098\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 13025\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"train/audio\", name=\"en-US\", split='train')\n",
    "data = data.filter(lambda example: \"_background_noise_\" not in example[\"audio\"][\"path\"])\n",
    "data = data.train_test_split(test_size=0.2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_paths(dataset, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for example in dataset:\n",
    "            file_path = example['audio']['path']\n",
    "            relative_path = os.path.relpath(file_path, start=os.getcwd())\n",
    "            file.write(relative_path.replace(\"\\\\\", \"/\") + '\\n')\n",
    "\n",
    "if SAVE_TRAIN_TEST:\n",
    "    save_file_paths(data['train'], 'train.txt')\n",
    "    save_file_paths(data['test'], 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'c:\\\\Users\\\\User\\\\Documents\\\\Studia\\\\dlm-2\\\\train\\\\audio\\\\sheila\\\\e8b6f6fe_nohash_0.wav',\n",
       "  'array': array([0.01608276, 0.02075195, 0.02236938, ..., 0.02816772, 0.02255249,\n",
       "         0.0229187 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 21}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_background_noise_',\n",
       " 'bed',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'down',\n",
       " 'eight',\n",
       " 'five',\n",
       " 'four',\n",
       " 'go',\n",
       " 'happy',\n",
       " 'house',\n",
       " 'left',\n",
       " 'marvin',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'off',\n",
       " 'on',\n",
       " 'one',\n",
       " 'right',\n",
       " 'seven',\n",
       " 'sheila',\n",
       " 'silence',\n",
       " 'six',\n",
       " 'stop',\n",
       " 'three',\n",
       " 'tree',\n",
       " 'two',\n",
       " 'up',\n",
       " 'wow',\n",
       " 'yes',\n",
       " 'zero']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data[\"train\"].features[\"label\"].names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bird'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "id2label[str(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\Lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db07f82927b746948c8c94330e9d6a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52098 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90942891b024e2083058210df95342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base\" if MODEL == Model.FacebookWav2Vec2 else \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "encoded_data = data.map(preprocess_function, remove_columns=\"audio\", batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283ceaf4b911453ab4f67cc36b9c64f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=encoded_data[\"test\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
